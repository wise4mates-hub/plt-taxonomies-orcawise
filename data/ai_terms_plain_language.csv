Term	Regulatory / technical definition	Plain‑language definition (indicative)	Source
AI system	Machine-based system inferring from inputs to generate outputs that influence environments.	A computer programme or gadget that uses collections of information and step-by-step rules to do tasks or make choices on its own. It powers things like chatbots, picture recognition and self-driving cars. These systems learn from examples and then act or reply when you feed in new information.	EU AI Act Art 3(1)
General Purpose AI model	Model competent across a broad range of tasks and adaptable for many downstream uses.	A very flexible AI “brain” trained on huge piles of information so it can handle many different jobs. For example, one large language model can write text, answer questions or translate. You can slot it into loads of uses because it isn’t built for just one task.	EU AI Act Rec 99
Provider	Person or org that develops or markets an AI system under its name or trademark.	The person or company that creates or sells an AI system. They must make sure the AI follows the rules and is safe to use. If you build or offer an AI tool, you are the provider and must keep users safe.	EU AI Act Art 3(5)
Deployer	Natural or legal person who uses an AI system under its authority.	The user or firm that takes an AI system and sets it up for real use. If you buy a medical-diagnosis AI and run it in your clinic, you are the deployer. You must check it works correctly and safely day-to-day.	EU AI Act Art 3(6)
Importer	Entity placing on EU market an AI system from a non‑EU provider.	A business that brings an AI product into a region (for example, the EU) from somewhere else. If you move an AI gadget from another country into your own market, you are the importer. You must be sure it obeys local rules before anyone uses it.	EU AI Act Art 3(10)
Distributor	Entity making an AI system available in EU without altering its function.	person or company that passes an AI product along the sales chain but did not build it. If your shop lists an AI tool made by someone else, you are the distributor. You must check the right paperwork and safety labels are in place and that no one has altered the product in a risky way.	EU AI Act Art 3(11)
Authorised representative	EU‑based proxy authorised to act on a non‑EU provider’s behalf.	A local agent who speaks to regulators for an overseas provider. If you sell AI in the EU but are based elsewhere, this representative handles the compliance paperwork with EU authorities.	EU AI Act Art 3(12)
Placing on the market	First supply of an AI system for distribution or use in the EU.	The first moment an AI product is offered for sale or supplied to users in a region. Think of it as launch day for the public.	EU AI Act Art 3(13)
Putting into service	First use of an AI system by the deployer for its intended purpose.	The point an AI system is actually switched on for its intended job. For example, once a new factory robot starts working on the line, it has been “put into service”.	EU AI Act Art 3(14)
High‑risk AI system	AI listed in Annex III or integrated into certain safety products, subject to strict rules.	AI whose errors or misuse could seriously harm people’s safety, health, rights or income. These tools sit in critical areas like medicine, transport or policing. Because the stakes are high, they must meet strict safety standards.	EU AI Act Art 6
Unacceptable‑risk AI practice	AI uses prohibited outright due to unacceptable harm (e.g. social scoring).	A use of AI that the law bans because it is clearly too harmful or unfair, such as secret opinion-manipulation, mass surveillance without cause or social-credit scoring.	EU AI Act Art 5
Limited‑risk AI	AI that must meet specific transparency duties (chatbots, emotion AI, deepfakes).	AI with some possible downsides but not enough to warrant a ban. A chatbot or mild content filter is in this class. You mainly have to tell users it is AI so they know what they are dealing with.	EU AI Act Art 50
Minimal‑risk AI	AI outside higher tiers with no special Act duties.	AI that presents little or no danger, such as spam filters or video-game characters. Normal consumer rules already cover these tools, so no extra law applies.	EU AI Act Rec 18
Risk‑management system	Continuous process to identify, analyse and mitigate AI risks.	A continuing process for spotting dangers an AI might cause and cutting them down. For high-risk AI, you must keep checking for things that could go wrong (like data errors) and fix them fast.	EU AI Act Art 9
Technical documentation	Information describing AI design, training, and purpose for regulators.	A clear written “manual” that shows how the AI was built, trained, tested and why it meets all rules.	EU AI Act Art 11
Record‑keeping	Automatic logging of AI operation to allow traceability.	Automatic logs that capture what the AI did and when. If something fails, you (or a regulator) can trace the cause.	EU AI Act Art 12
Human oversight	Measures enabling humans to monitor, understand and intervene in AI operation.	Designing the AI so a person can watch, understand and step in or shut it off if needed. This human safety net helps prevent serious mistakes.	EU AI Act Art 14
Accuracy, robustness & cybersecurity	AI must perform correctly, resist errors and attacks.	The AI must give correct answers (accuracy), keep working even with weird inputs (robustness), and resist hackers (cybersecurity). Together these make the system reliable and secure.	EU AI Act Art 15
Conformity assessment	Procedure verifying high‑risk AI meets Act requirements before market.	A formal safety check, often by an approved outside tester, to prove the AI meets legal rules before launch.	EU AI Act Art 43
Notified body	Independent organisation authorised to conduct conformity assessments.	An independent test house recognised by regulators to carry out that safety check and issue a pass or fail.	EU AI Act Art 3(22)
EU declaration of conformity	Provider’s statement that the AI complies with the Act.	A signed promise from the provider stating, “This product meets every relevant EU rule”.	EU AI Act Art 48
CE marking for AI	Symbol affixed to high‑risk AI once conformity confirmed.	The little “CE” logo on the product that shows the above promise has been made and the conformity check done.	EU AI Act Art 49
Post‑market monitoring	Provider’s plan to watch the AI’s real‑world performance and risks.	The provider’s work to watch how the AI performs once the public starts using it, spot problems early and put them right.	EU AI Act Art 61
Serious incident	Event causing death, serious harm, or fundamental‑rights breach linked to AI.	A major failure where an AI system directly or indirectly causes death, serious injury, large financial loss, or a serious breach of rights. One example is an AI-controlled medicine pump that gives a lethal overdose to a patient. Such incidents must be reported quickly and examined so the cause is fixed and future harm is avoided.	EU AI Act Art 3(44)
Corrective action	Steps a provider must take when AI breaches requirements or causes risk.	The measures you take once a fault is found in an AI system. This could mean updating the software, limiting where the tool is used, or taking it off the market entirely. The aim is to stop more harm and make sure the problem cannot return.	EU AI Act Art 21
Personal data	Any information about an identified or identifiable person.	Any information that can identify a living person, for example a name, photo, phone number, or even a clear opinion about someone. If a piece of data lets you single out who the person is, it counts as personal data. Extra care and legal rules apply when an AI handles it.	GDPR Art 4(1)
Processing	Any operation performed on personal data (collection, storage, etc.).	Every action done to personal data, including collecting, storing, sharing, analysing, or deleting it. Whenever an AI or organisation touches personal data, that is processing. The law sets rules for each stage.	GDPR Art 4(2)
Controller	Entity deciding why and how personal data are processed.	The organisation that decides why and how personal data will be processed. It is in charge of meeting privacy laws and keeping the data safe. A hospital using patient records to give treatment is a controller.	GDPR Art 4(7)
Processor	Entity processing data on behalf of a controller.	A separate party that handles personal data only under the controller’s written instructions, for example a cloud-storage firm hired to keep patient files. The processor must follow what the controller says and keep the data secure. It cannot decide new uses on its own.	GDPR Art 4(8)
Profiling	Automated processing to analyse or predict personal aspects.	An AI analysing personal data to predict things about a person, such as buying habits or likely credit risk. It groups people by patterns and guesses future behaviour. This can bring benefits, but it also raises fairness and privacy questions.	GDPR Art 4(4)
Automated decision‑making	Decision based solely on automated processing producing legal or similar effects.	Letting a computer system decide something important about a person without a human checking it first, for example approving a loan or short-listing job applicants. The system uses data and predefined rules to reach its decision. Some laws give people the right to human review of such outcomes.	GDPR Art 22
Pseudonymisation	Processing so data can’t be linked to a person without extra info kept separately.	Replacing clear identifiers (like names) with codes so the data no longer points straight to a person unless you also hold the key. If the coded list is leaked, outsiders cannot tell whose data it is. This lowers privacy risks while still allowing analysis.	GDPR Art 4(5)
Biometric data	Personal data from tech processing of physical traits enabling unique ID.	Body or behaviour traits that can identify someone, such as fingerprints, facial images, voice patterns, or the way a person walks. Because these traits are unique, they are highly sensitive. Special safeguards apply when AI systems use them.	GDPR Art 4(14)
Consent	Freely given, specific, informed and unambiguous indication of wishes.	A person’s clear and freely given “yes” that lets you collect or use their personal data. They must understand what will happen to their information and be able to change their mind. Tick-boxes that are pre-filled or unclear do not count as real consent.	GDPR Art 4(11)
Data minimisation	Only data necessary for the stated purpose may be processed.	Collect only the personal data that you truly need for a task, nothing extra. If an app only needs your email to work, it should not also ask for your date of birth. Smaller data collections reduce privacy risk and storage cost.	GDPR Art 5(1)(c)
Recommender system	Algorithm suggesting or ranking content based on parameters.	Software that studies what you or similar users like and suggests films, songs, news, or products you might enjoy next. It powers features such as “People who bought this also bought…”. Good recommender design should avoid trapping you in a narrow bubble.	DSA Art 27
Very Large Online Platform (VLOP)	Service with ≥45 million EU users, facing extra duties.	A digital service with at least forty-five million users in the European Union, for example a major social network or marketplace. Because of its reach, a VLOP must follow extra rules on safety, transparency, and user rights. Regulators can fine it if it does not.	DSA Art 33
Systemic risk	Wide societal harms a platform’s services might create or spread.	A threat that can spread across society or the whole economy, not just affect one user. A powerful misinformation-spreading AI is one example, because it can undermine elections or public health at scale. Systems with systemic risk often face stricter oversight.	DSA Art 34
Advertising transparency	Real‑time disclosure of ad, advertiser and targeting reasons.	Clear labelling that a message is a paid advert and an explanation of why you personally were shown it. This lets people understand when content tries to sell or influence them. It also helps spot misleading or unfair targeting.	DSA Art 26
User choice for recommender	Platform must offer at least one non‑profiling or chronological feed option.	Giving each user simple controls over how content suggestions are ranked or letting them turn personalisation off. Examples include a “recent posts” setting or an option to see recommendations based only on general popularity. Choice helps users avoid unwanted bias or filter bubbles.	DSA Art 27(3)
Algorithmic repository	Public database of all ads served by a VLOP.	A public register where an organisation lists its automated decision systems and brief details of how they work. Regulators and citizens can look up which algorithms are in use and for what purpose. This supports accountability.	DSA Art 39
Safety, security & robustness	AI should function reliably and resist risks throughout lifecycle.	Three pillars for trustworthy AI. Safety means the system should not harm people or property. Security means it resists hacking or tampering. Robustness means it keeps working even when inputs are odd or conditions change.	UK AI White Paper Principle 1
Appropriate transparency & explainability	AI operations and decisions should be understandable to affected people.	Making sure people know that AI is involved and can understand, in plain language, why it reached a given result. This may include a short explanation screen or a link to more detail for experts. Transparency builds trust and lets errors be spotted.	UK AI White Paper Principle 2
Fairness (UK)	Avoid unlawful discrimination and ensure equitable outcomes.	Designing and running AI so that similar people receive similar outcomes and protected groups are not disadvantaged. For instance, a hiring tool should not favour one gender over another unless the job genuinely requires it. UK guidance treats fairness as a core principle alongside safety and accountability.	UK AI White Paper Principle 3
Accountability & governance	Clear responsibility and oversight for AI outcomes.	Clear structures that show who is responsible for each AI system, how it is monitored, and how decisions are reviewed. This can include roles, internal policies, regular audits, and reporting lines to senior management. Strong governance ensures someone is answerable if the AI goes wrong.	UK AI White Paper Principle 4
Contestability & redress	Routes for people to dispute and remedy harmful AI decisions.	Your right to question an AI decision and to have mistakes corrected or harms compensated. That may involve a human appeal process, error logs you can request, or financial redress schemes. These rights give people confidence that they are not at the mercy of a black-box.	UK AI White Paper Principle 5
Transparency (ISO)	Appropriate information about AI system made available to stakeholders.	Under ISO standards, giving enough information about an AI’s design, data, and purpose so users and auditors can see what the system does. It covers documentation, user notices, and access for inspectors. Transparency helps verify compliance with safety and ethics rules.	ISO/IEC 22989 §3.1.5
Explainability (ISO)	Property enabling humans to understand factors influencing an AI output.	Ensuring that an AI can provide understandable reasons for its outputs. This might be a short plain-language summary for users or a detailed trace for specialists. Explainability supports trust and easier debugging.	ISO/IEC 22989 §3.1.8
Robustness (ISO)	Ability of AI to maintain performance under any circumstances.	The AI’s ability to keep performing well even when facing new or slightly corrupted data, hardware faults, or attempts to game the system. Robust systems include safeguards like input checks and fallback modes. ISO treats robustness as a core technical quality.	ISO/IEC 22989 §3.1.11
Trustworthiness	AI meets stakeholder expectations in a verifiable way.	A blend of lawfulness, ethics, and technical soundness that lets people rely on an AI system. Trustworthy AI hits all the key points: fairness, safety, security, transparency, and accountability. Without this blend, uptake and public confidence suffer.	ISO/IEC 22989 §3.1.12
Stakeholder	Party that can affect or be affected by AI system.	Anyone who can affect or is affected by an AI system. This group includes users, people whose data are processed, company staff, regulators, and the wider public. Good AI projects consult stakeholders early and often.	ISO/IEC 22989 §3.3
AI Management System (AIMS)	Organisational framework for responsible AI development and use.	A company-wide set of policies, roles, and procedures for planning, running, and reviewing every AI tool in use. It works like quality management but for AI, covering data standards, risk checks, and training for staff. A mature AIMS shows regulators that AI risks are under control.	ISO/IEC 42001
AI impact assessment	Structured evaluation of societal and ethical impacts of AI.	A forward-looking study that asks who the system might help or harm and how to reduce any harm before launch. It reviews privacy, bias, safety, and social effects. The findings guide design changes and monitoring plans.	ISO/IEC 42005 draft
Risk management (ISO)	Coordinated activities to direct and control AI risks.	The standard cycle of spotting risks, weighing their seriousness, acting to reduce them, and checking results over time. ISO advises building this into every phase of AI development. Continuous risk management keeps systems safe as conditions change.	ISO/IEC 23894
AI lifecycle	Stages from inception, development, deployment to retirement.	All the stages of an AI system’s life: idea, design, data gathering, training, testing, deployment, updates, and retirement. Each stage has its own checks for safety, fairness, and quality. Managing the full lifecycle prevents problems slipping through.	ISO/IEC 22989 Clause 6
Bias (ISO TR 24027)	Systematic error producing unfair outcomes.	A built-in pattern that unfairly favours or harms certain groups, often caused by skewed data or design choices. For example, a crime-prediction tool may over-target areas where police already patrol more often. Detecting and fixing bias protects people’s rights.	ISO/IEC TR 24027
Data quality (AI)	Degree to which data characteristics meet AI requirements (accuracy, representativeness).	How accurate, complete, and relevant the information is for the AI’s job. Good data quality helps the model learn the right lessons and keeps errors low. Poor data quality leads straight to poor AI output.	ISO/IEC 5259‑1
Accuracy (AI quality)	Closeness of AI outputs to correct values.	The share of answers an AI gets right when compared with the ground truth. High accuracy means few mistaken predictions. It is one key measure of performance.	ISO/IEC 25059
Safety (AI quality)	Avoidance of unacceptable risk of harm from AI.	The system’s ability to avoid causing physical, financial, or social harm, supported by testing and fail-safes. Safety checks must continue after launch. Users need clear instructions and emergency stops.	ISO/IEC 25059
Security (AI quality)	Protection of AI from malicious attacks or misuse.	How well the AI and its data are shielded from unauthorised access, sabotage, or manipulation. Measures include encryption, access controls, and regular security updates. Good security protects users and the provider’s reputation.	ISO/IEC 25059
Predictability	Property enabling reliable assumptions about AI output.	The system behaves consistently in the same situation, making its actions easier to foresee and trust. Lack of predictability can confuse users and mask hidden faults. Test suites and monitoring improve predictability.	ISO/IEC 22989 §3.1.9
Artificial intelligence (US)	Machine‑based system that makes predictions, recommendations, or decisions.	Technology that lets machines carry out tasks needing human-style understanding, such as recognising speech, spotting objects, or planning routes. The US definition is broad and covers learning and rule-based systems. Policy documents often focus on rights and safety when these systems affect people.	US National AI Initiative Act §9401
Automated decision system (ADS)	Algorithm using data analytics to make or support governmental decisions.	Software that makes or supports a choice which has legal or similar impact on a person, for example granting benefits or setting parole conditions. In some US states, agencies must publicly list and justify any ADS they use. Transparency helps guard against hidden discrimination.	Vermont ADS Statute 2019
Generative AI	AI models that emulate input patterns to create new synthetic content.	AI that produces new content, from text and images to music or code, based on patterns it has learned. Examples include chatbots that draft emails or models that create artwork from prompts. Policies often ask providers to label AI-generated material.	US Senate draft 2024
High‑impact system (Canada)	AI meeting criteria for significant effects set by regulation.	An automated tool whose errors could seriously affect someone’s health, safety, or fundamental rights. Such systems face extra rules under Canada’s AI Act, including risk assessments and public reporting. The goal is to prevent large-scale harm.	Canada Bill C‑27
Biased output (Canada)	AI output that unjustifiably differentiates individuals on protected grounds.	An AI result that treats groups unfairly because of built-in prejudice. The Canadian Act requires measures to test for and correct biased output. Regular audits and diverse data help reduce this risk.	Canada Bill C‑27
Safe & effective system (US)	AI that performs as intended without undue risk.	An automated tool that achieves its stated aim without creating unreasonable danger. For example, a medical AI should diagnose accurately and not recommend harmful treatments. US agencies may demand proof of both safety and effectiveness before approval.	US AI Bill of Rights 2022
Algorithmic discrimination	Unlawful differential treatment caused by automated processing.	Unequal treatment caused by an AI’s design or data, usually harming protected groups. Anti-discrimination laws can apply even if nobody intended bias. Organisations are expected to test, spot, and fix such problems.	US EEOC/FTC Joint Statement 2023
Data privacy (AI Bill of Rights)	Protection of users’ personal data in AI systems.	The principle that people control how data about them are collected, used, and shared when AI is involved. Steps include clear notices, strict access controls, and safe deletion. Privacy builds public trust.	US AI Bill of Rights 2022
Reliability (AI quality)	Ability of AI to maintain performance over time under stated conditions.	The AI keeps working correctly under normal conditions, day after day. Reliable systems have low downtime and stable results. Planned maintenance and monitoring keep reliability high.	ISO/IEC 25059
Resilience	Capacity of AI to recover quickly from disruptions or partial failures.	The AI can recover quickly from errors, attacks, or failures and keep providing its service. Features such as backups and graceful-degradation modes support resilience. This is vital for critical sectors like healthcare.	ISO/IEC 25059
Availability	AI readiness for correct service when requested.	The AI system is accessible and ready when users need it, often measured as a percentage of uptime. High availability is key for services such as emergency response or online banking. Cloud redundancy helps achieve this.	ISO/IEC 25059
Integrity	Protection against unauthorised modification of AI outputs or data.	Data and functions stay accurate and unaltered unless a person with permission makes a change. Integrity checks like digital signatures detect tampering early. Good integrity safeguards trust in the results.	ISO/IEC 25059
Usability	Extent to which AI can be used easily and effectively by specified users.	People can learn and operate the AI easily, with clear instructions and feedback. Good usability reduces mistakes and boosts adoption. Designers run user testing to spot obstacles.	ISO/IEC 25059
Maintainability	Ease with which AI can be modified to correct faults or improve performance.	Engineers can update, debug, and improve the AI without huge cost or downtime. Clean code, modular design, and thorough documentation support maintainability. This keeps the system useful over its whole life.	ISO/IEC 25059
Portability	Ability of AI to be transferred to another environment.	The AI can move to new hardware, cloud services, or operating systems with minimal work. Containerisation and standard interfaces make portability easier. This flexibility avoids vendor lock-in.	ISO/IEC 25059
Completeness (data)	Degree to which required data are present.	The dataset covers every case the AI needs to understand, with no key gaps. Missing data can skew learning and decisions. Regular checks ensure completeness.	ISO/IEC 5259‑2
Representativeness	Extent data reflects the target population.	The dataset reflects the real-world variety the AI will face, preventing it from learning a narrow or biased view. Sampling methods and diversity reviews help achieve this. Representativeness supports fairness and accuracy.	ISO/IEC 5259‑2
Consistency (data)	Absence of contradictions within data sets.	Data follow the same formats and rules throughout, for example dates all written day-month-year. Consistent data reduce processing errors and simplify analysis. Automated validators can enforce consistency.	ISO/IEC 5259‑2
Relevance (data)	Degree to which data meets the needs of the AI task.	Each piece of data matters to the AI’s task and unwanted extras are removed. Irrelevant data add noise and slow training. Clear data-selection criteria protect relevance.	ISO/IEC 5259‑2
Timeliness	Delay between data availability and its use.	Data arrive fast enough to be useful for the goal, such as traffic information updated every minute. Out-of-date data can mislead the AI. Automated feeds and caching strategies support timeliness.	ISO/IEC 5259‑2
Currentness	Extent data is up‑to‑date.	The dataset is refreshed often so it matches present-day reality, for example language models updated with new slang. Stale data degrade performance. Scheduled re-training maintains currentness.	ISO/IEC 5259‑2
Identifiability (data)	Ability to uniquely label an entity in data.	How easily someone can pick out a real person from the dataset. Reducing identifiability through masking or aggregation protects privacy. Risk assessments gauge the remaining chance of re-identification.	ISO/IEC 5259‑2
Credibility (data)	Trustworthiness and reliability of data source.	The data come from trustworthy sources and have been checked for errors. Credible data underpin believable AI outputs. Source vetting and fact-checking raise credibility.	ISO/IEC 5259‑2
Auditability (data)	Ease of tracing data lineage and processing.	Clear records show when, where, and how the data were gathered or changed. This trace lets reviewers verify compliance and spot mistakes. Good audit logs are tamper-evident.	ISO/IEC 5259‑2
Balance (data)	Equitable distribution across classes or groups.	No group or outcome dominates the dataset so that the AI learns a fair view. Balanced data help prevent bias. Weighting or additional sampling can correct imbalances.	ISO/IEC 5259‑2
Training data	Data used to fit AI model parameters.	Examples the AI studies to learn its patterns, often paired with the correct answers. Good training data are varied, balanced, and accurate. They form the foundation of model quality.	EU AI Act Art 3(17)
Validation data	Data used to tune AI model hyper‑parameters.	A separate set used during training to check whether the model is improving and to tune its settings. Validation guards against over-fitting, where the model just memorises the training examples. Balanced validation data reveal hidden bias early.	EU AI Act Art 3(18)
Testing data	Data used to evaluate final performance.	Fresh examples kept aside until training is finished, used to measure how well the AI performs on truly unseen cases. Strong test results suggest the model will work in the real world. Testing data must match the conditions the AI will face.	EU AI Act Art 3(19)
Input data	Data provided to AI system to produce an output.	The information fed into the AI each time it runs, such as a customer’s loan application. The system processes this input to produce its answer. Clear input formats reduce errors.	EU AI Act Art 3(20)
Output	Result produced by an AI system.	The AI’s result, for example an approved credit limit or a product recommendation. Users and regulators must be able to understand and challenge the output if needed.	EU AI Act Art 3(21)
Intended purpose	Use for which the AI is designed.	The specific job the AI was built to do, described clearly before launch. Using the tool for a different purpose can create new risks and may need fresh approval. Providers should state the intended purpose in user manuals.	EU AI Act Art 3(23)
Performance	Ability of AI to achieve intended purpose.	How well, how quickly, and how consistently the AI achieves its intended purpose in real conditions. Measures include accuracy, response time, and user satisfaction. Performance tests continue after release.	EU AI Act Art 3(24)
Harmonised standard	EU standard giving presumption of conformity.	An EU-approved technical rule. If a provider follows it fully, regulators presume the product meets the law, simplifying compliance checks.	EU AI Act Art 3(26)
Common specification	EU‑level doc with technical solutions when no harmonised standard exists.	An EU guidance document issued when no harmonised standard exists yet. It offers detailed steps to meet legal requirements until a formal standard is published.	EU AI Act Art 3(27)
Market surveillance authority	National body ensuring AI Act enforcement.	A public body in each EU country that checks products already on sale, including AI systems, to be sure they follow safety, fairness, and labelling rules. The authority can demand fixes or remove unsafe products.	EU AI Act Art 63
Post‑market monitoring plan	Structured scheme for providers to collect and analyse AI performance data.	A provider’s written plan for tracking an AI system once it is in use, collecting feedback, logging errors, and issuing timely patches. Good plans set clear roles, data sources, and action thresholds.	EU AI Act Art 61
Digital Services Coordinator	National authority supervising DSA compliance.	The national body that enforces the EU Digital Services Act in each member state. It coordinates with other countries and can fine online platforms that ignore safety or transparency rules.	DSA Art 49
Trusted flagger	Entity whose content‑takedown notices platforms must act on with priority.	A vetted individual or organisation whose reports of illegal or harmful content online must be handled quickly by platforms. Trusted flaggers earn this status through proven expertise and accuracy.	DSA Art 22
Crisis protocol	Procedure VLOPs must follow to mitigate serious crises impacts.	A set of emergency measures large platforms must follow during events like pandemics or wars. The protocol might require boosting official information and slowing the spread of rumours.	DSA Art 36
Adaptivity	Characteristic of AI that changes its behaviour based on data.	The AI’s ability to learn from new data and refine its behaviour over time without a full rebuild. Adaptive systems stay useful as conditions shift.	UK AI White Paper
Autonomy	AI operates with limited human intervention.	How far the AI can make and carry out decisions on its own, for example a drone that navigates without a pilot. Higher autonomy often demands stronger safeguards.	UK AI White Paper
Notice & explanation	Principle that users should be aware of AI use and receive understandable reasons.	Telling users that an AI system is making a decision about them and providing a clear reason they can understand. This empowers people to accept, question, or appeal the outcome.	US AI Bill of Rights
Human alternatives fallback	Option for people to opt out of AI decision and get human review.	Offering a way for users to ask a human to review or overrule the AI’s decision if it seems wrong or unfair.	US AI Bill of Rights
Security assessment (China)	Regulator‑mandated review of AI risks to national security and social order.	A government review to ensure certain AI systems, such as content generators or data-analysis tools, do not threaten national security or public order before they are launched.	China GenAI Measures Art 17
Core socialist values compliance	Requirement that AI outputs align with stated Chinese values.	A Chinese rule that AI output must not conflict with officially approved social and moral principles, for instance it must respect national unity and avoid forbidden speech.	China GenAI Measures Art 4
Content filtering	Mechanism to block illegal or harmful AI outputs.	Automated checking of text, images, or videos to block or label material that violates laws or platform rules, like hate speech or extremist content. Effective filters balance safety with freedom of expression.	China Deep Synthesis Rules
